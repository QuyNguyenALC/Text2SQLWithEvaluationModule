{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# from trulens.core.Feedback import Groundedness, Relevance, ContextRelevance\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# from trulens.feedback.feedback import Groundedness, Relevance, ContextRelevance\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrulens\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproviders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI \u001b[38;5;28;01mas\u001b[39;00m TruOpenAI\n\u001b[0;32m---> 22\u001b[0m openai_provider \u001b[38;5;241m=\u001b[39m \u001b[43mTruOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# from trulens_eval.feedback.provider import OpenAI\u001b[39;00m\n\u001b[1;32m     24\u001b[0m TRULENS_AVAILABLE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/trulens/providers/openai/provider.py:69\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, endpoint, pace, rpm, model_engine, *args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m self_kwargs\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     67\u001b[0m self_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_engine\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model_engine\n\u001b[0;32m---> 69\u001b[0m self_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mendpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mopenai_endpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpenAIEndpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrpm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrpm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mself_kwargs\n\u001b[1;32m     75\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/trulens/providers/openai/endpoint.py:341\u001b[0m, in \u001b[0;36mOpenAIEndpoint.__init__\u001b[0;34m(self, client, rpm, pace, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_register_instance\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    338\u001b[0m         \u001b[38;5;66;03m# This argument is not allowed by the `openai.OpenAI`\u001b[39;00m\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;66;03m# constructor.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_register_instance\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 341\u001b[0m     client \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m     self_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OpenAIClient(client\u001b[38;5;241m=\u001b[39mclient)\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/openai/_client.py:126\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    124\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    128\u001b[0m     )\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Text-to-SQL Processor Module - TruLens Integration\n",
    "Thay thế DeepEval bằng TruLens để đánh giá chất lượng SQL\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Cài đặt TruLens\n",
    "try:\n",
    "    # from trulens_eval import Tru, Feedback\n",
    "    from trulens.core import TruSession\n",
    "    from trulens.core import Feedback, Select\n",
    "    # from trulens.core.Feedback import Groundedness, Relevance, ContextRelevance\n",
    "    # from trulens.feedback.feedback import Groundedness, Relevance, ContextRelevance\n",
    "    from trulens.providers.openai import OpenAI as TruOpenAI\n",
    "    openai_provider = TruOpenAI()\n",
    "    # from trulens_eval.feedback.provider import OpenAI\n",
    "    TRULENS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TRULENS_AVAILABLE = False\n",
    "\n",
    "class Text2SQLProcessor:\n",
    "    \"\"\"Processes natural language queries to SQL using Gemini, FAQ matching, and TruLens.\"\"\"\n",
    "    \n",
    "    def __init__(self, faq_path: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize the Text2SQL processor\n",
    "        \n",
    "        Args:\n",
    "            faq_path: Path to the FAQ JSON file (optional)\n",
    "        \"\"\"\n",
    "        self.faq_data = self._load_faq(faq_path) if faq_path else []\n",
    "        self.ambiguous_terms = [\"best\", \"popular\", \"recent\", \"top\", \"important\", \"tốt nhất\", \"phổ biến\", \"gần đây\", \"hàng đầu\", \"quan trọng\"]\n",
    "        \n",
    "        # Configure Gemini if API key is available\n",
    "        api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "        self.gemini_available = False\n",
    "        if api_key:\n",
    "            try:\n",
    "                genai.configure(api_key=api_key)\n",
    "                self.gemini_available = True\n",
    "            except Exception as e:\n",
    "                print(f\"Error configuring Gemini: {str(e)}\")\n",
    "        \n",
    "        # Configure TruLens if available\n",
    "        self.openai_api_key_available = os.environ.get(\"OPENAI_API_KEY\") is not None\n",
    "        self.trulens_available = TRULENS_AVAILABLE and self.openai_api_key_available\n",
    "        \n",
    "        if self.trulens_available:\n",
    "            try:\n",
    "                # Khởi tạo TruLens\n",
    "                self.tru = TruSession()\n",
    "                \n",
    "                # Khởi tạo các feedback functions\n",
    "                self.relevance = Feedback(openai_provider.relevance_with_cot_reasons, name=\"Answer Relevance\").on_input().on_output()\n",
    "                self.groundedness = Feedback(openai_provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\").on(Select.RecordCalls.retrieve.rets.collect()).on_output()\n",
    "                self.context_relevance = Feedback(openai_provider.context_relevance_with_cot_reasons, name=\"Context Relevance\").on_input().on(Select.RecordCalls.retrieve.rets.collect()) .aggregate(np.mean)\n",
    "                \n",
    "                # Custom feedback function cho SQL correctness\n",
    "                self.sql_correctness = Feedback(\n",
    "                    prompt_template=\"Evaluate if the SQL query correctly translates the user's question and follows SQL best practices for the given schema. Question: {question} Schema: {schema} SQL: {sql} Rate from 0 to 1, where 1 is perfect.\",\n",
    "                    name=\"sql_correctness\"\n",
    "                )\n",
    "                \n",
    "                # Custom feedback function cho SQL syntax\n",
    "                self.sql_syntax = Feedback(\n",
    "                    prompt_template=\"Check if the SQL query has correct syntax. SQL: {sql} Rate from 0 to 1, where 1 is perfect syntax.\",\n",
    "                    name=\"sql_syntax\"\n",
    "                )\n",
    "                \n",
    "                # Thiết lập ngưỡng đánh giá\n",
    "                self.thresholds = {\n",
    "                    \"relevance\": 0.7,\n",
    "                    \"groundedness\": 0.7,\n",
    "                    \"context_relevance\": 0.7,\n",
    "                    \"correctness\": 0.6,\n",
    "                    \"syntax\": 0.8\n",
    "                }\n",
    "                \n",
    "                print(\"TruLens initialized successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error initializing TruLens: {str(e)}\")\n",
    "                self.trulens_available = False\n",
    "        \n",
    "        # Debug info\n",
    "        print(\"TruLens Debug Info:\")\n",
    "        print(f\"- TRULENS_AVAILABLE: {TRULENS_AVAILABLE}\")\n",
    "        print(f\"- OpenAI API Key available: {self.openai_api_key_available}\")\n",
    "        print(f\"- TruLens metrics available: {self.trulens_available}\")\n",
    "        if not TRULENS_AVAILABLE:\n",
    "            print(\"  - Reason: TruLens library not installed or import failed\")\n",
    "        elif not self.openai_api_key_available:\n",
    "            print(\"  - Reason: OpenAI API key not set in environment variables\")\n",
    "    \n",
    "    def _load_faq(self, faq_path: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Load FAQ data from JSON file\n",
    "        \n",
    "        Args:\n",
    "            faq_path: Path to the FAQ JSON file\n",
    "            \n",
    "        Returns:\n",
    "            List of FAQ entries\n",
    "        \"\"\"\n",
    "        if not faq_path or not os.path.exists(faq_path):\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            with open(faq_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                return data.get(\"questions\", [])\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading FAQ file: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def match_faq(self, query: str) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Match query against FAQ entries\n",
    "        \n",
    "        Args:\n",
    "            query: Natural language query\n",
    "            \n",
    "        Returns:\n",
    "            Matched FAQ entry or None\n",
    "        \"\"\"\n",
    "        if not self.faq_data:\n",
    "            return None\n",
    "        \n",
    "        # Simple exact matching for now\n",
    "        for item in self.faq_data:\n",
    "            for pattern in item.get(\"patterns\", []):\n",
    "                if pattern.lower() == query.lower():\n",
    "                    return item\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def detect_ambiguity(self, query: str, schema: str) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Detect ambiguity in the query\n",
    "        \n",
    "        Args:\n",
    "            query: Natural language query\n",
    "            schema: Database schema as string\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (is_ambiguous, ambiguity_analysis)\n",
    "        \"\"\"\n",
    "        # Simple term-based detection\n",
    "        for term in self.ambiguous_terms:\n",
    "            if term in query.lower():\n",
    "                return True, f\"Ambiguous term detected: '{term}'\"\n",
    "        \n",
    "        # Use Gemini for more sophisticated detection if available\n",
    "        if self.gemini_available:\n",
    "            try:\n",
    "                prompt = f\"\"\"\n",
    "                Analyze the following query and determine if it is ambiguous in the context of the given database schema.\n",
    "                If it is ambiguous, explain why. If it is clear, respond with \"The query is clear.\"\n",
    "                \n",
    "                Database Schema:\n",
    "                {schema}\n",
    "                \n",
    "                Query: \"{query}\"\n",
    "                \n",
    "                Analysis:\n",
    "                \"\"\"\n",
    "                \n",
    "                model = genai.GenerativeModel('gemini-1.5-pro')\n",
    "                response = model.generate_content(prompt)\n",
    "                \n",
    "                if \"ambiguous\" in response.text.lower() or \"unclear\" in response.text.lower():\n",
    "                    return True, response.text\n",
    "                \n",
    "                return False, None\n",
    "            except Exception as e:\n",
    "                print(f\"Error using Gemini for ambiguity detection: {str(e)}\")\n",
    "        \n",
    "        # Default to non-ambiguous if Gemini is not available\n",
    "        return False, None\n",
    "    \n",
    "    def generate_clarification_questions(self, query: str, ambiguity_analysis: str, schema: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate clarification questions for ambiguous queries\n",
    "        \n",
    "        Args:\n",
    "            query: Natural language query\n",
    "            ambiguity_analysis: Analysis of the ambiguity\n",
    "            schema: Database schema as string\n",
    "            \n",
    "        Returns:\n",
    "            Clarification questions as string\n",
    "        \"\"\"\n",
    "        if not self.gemini_available:\n",
    "            # Default questions if Gemini is not available\n",
    "            return \"Could you please clarify your query? What specific information are you looking for?\"\n",
    "        \n",
    "        try:\n",
    "            prompt = f\"\"\"\n",
    "            Based on the following ambiguity analysis of a query, generate 1-2 clear, concise questions to help clarify the user's intent.\n",
    "            For each question, provide 2-3 specific options to choose from.\n",
    "            \n",
    "            Database Schema:\n",
    "            {schema}\n",
    "            \n",
    "            Query: \"{query}\"\n",
    "            \n",
    "            Ambiguity Analysis: {ambiguity_analysis}\n",
    "            \n",
    "            Format your response as:\n",
    "            1. [Question]\n",
    "               - [Option 1]\n",
    "               - [Option 2]\n",
    "               - [Option 3]\n",
    "            \n",
    "            Clarification Questions:\n",
    "            \"\"\"\n",
    "            \n",
    "            model = genai.GenerativeModel('gemini-1.5-pro')\n",
    "            response = model.generate_content(prompt)\n",
    "            \n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating clarification questions: {str(e)}\")\n",
    "            return \"Could you please clarify your query? What specific information are you looking for?\"\n",
    "    \n",
    "    def update_query_with_clarification(self, original_query: str, clarification_responses: Dict[str, str]) -> str:\n",
    "        \"\"\"\n",
    "        Update query based on clarification responses\n",
    "        \n",
    "        Args:\n",
    "            original_query: Original natural language query\n",
    "            clarification_responses: Dict of clarification responses\n",
    "            \n",
    "        Returns:\n",
    "            Updated query\n",
    "        \"\"\"\n",
    "        if not self.gemini_available:\n",
    "            # Simple concatenation if Gemini is not available\n",
    "            clarifications = \", \".join([f\"{k}: {v}\" for k, v in clarification_responses.items()])\n",
    "            return f\"{original_query} (clarified: {clarifications})\"\n",
    "        \n",
    "        try:\n",
    "            prompt = f\"\"\"\n",
    "            Based on the original query and the user's clarification responses, create a clear, unambiguous query.\n",
    "            \n",
    "            Original Query: \"{original_query}\"\n",
    "            \n",
    "            Clarification Responses:\n",
    "            {json.dumps(clarification_responses, indent=2)}\n",
    "            \n",
    "            Updated Query:\n",
    "            \"\"\"\n",
    "            \n",
    "            model = genai.GenerativeModel('gemini-1.5-pro')\n",
    "            response = model.generate_content(prompt)\n",
    "            \n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating query with clarification: {str(e)}\")\n",
    "            clarifications = \", \".join([f\"{k}: {v}\" for k, v in clarification_responses.items()])\n",
    "            return f\"{original_query} (clarified: {clarifications})\"\n",
    "    \n",
    "    def clean_sql(self, sql_text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean SQL text by removing markdown formatting and extra whitespace\n",
    "        \n",
    "        Args:\n",
    "            sql_text: SQL text that may contain markdown formatting\n",
    "            \n",
    "        Returns:\n",
    "            Clean SQL text\n",
    "        \"\"\"\n",
    "        # Remove markdown code block markers (```sql, ```, etc.)\n",
    "        sql_text = re.sub(r'```\\w*\\s*', '', sql_text)\n",
    "        sql_text = re.sub(r'```\\s*$', '', sql_text)\n",
    "        \n",
    "        # Remove any leading/trailing whitespace\n",
    "        sql_text = sql_text.strip()\n",
    "        \n",
    "        # Remove any trailing semicolons (optional, depends on your SQL engine)\n",
    "        # sql_text = re.sub(r';+\\s*$', '', sql_text)\n",
    "        \n",
    "        return sql_text\n",
    "    \n",
    "    def generate_sql(self, query: str, schema: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate SQL from natural language query\n",
    "        \n",
    "        Args:\n",
    "            query: Natural language query\n",
    "            schema: Database schema as string\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing SQL and explanation\n",
    "        \"\"\"\n",
    "        if not self.gemini_available:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": \"Gemini API is not available. Please set the GOOGLE_API_KEY environment variable.\"\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            prompt = f\"\"\"\n",
    "            You are a Text-to-SQL conversion expert. Convert the following natural language query into a valid SQL query based on the provided schema.\n",
    "            Also provide a clear explanation of how the SQL query works.\n",
    "            \n",
    "            Database Schema:\n",
    "            {schema}\n",
    "            \n",
    "            Query: \"{query}\"\n",
    "            \n",
    "            Respond in the following format:\n",
    "            SQL: <your SQL query>\n",
    "            Explanation: <your explanation>\n",
    "            \"\"\"\n",
    "            \n",
    "            model = genai.GenerativeModel('gemini-1.5-pro')\n",
    "            response = model.generate_content(prompt)\n",
    "            \n",
    "            # Parse the response to extract SQL and explanation\n",
    "            text = response.text\n",
    "            sql = \"\"\n",
    "            explanation = \"\"\n",
    "            \n",
    "            if \"SQL:\" in text:\n",
    "                parts = text.split(\"SQL:\", 1)\n",
    "                if len(parts) > 1:\n",
    "                    sql_and_explanation = parts[1].strip()\n",
    "                    if \"Explanation:\" in sql_and_explanation:\n",
    "                        sql_parts = sql_and_explanation.split(\"Explanation:\", 1)\n",
    "                        sql = sql_parts[0].strip()\n",
    "                        explanation = sql_parts[1].strip()\n",
    "                    else:\n",
    "                        sql = sql_and_explanation\n",
    "            \n",
    "            # Clean SQL from markdown formatting\n",
    "            sql = self.clean_sql(sql)\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"sql\": sql,\n",
    "                \"explanation\": explanation\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"Error generating SQL: {str(e)}\"\n",
    "            }\n",
    "    \n",
    "    def evaluate_sql(self, query: str, sql: str, schema: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate SQL using TruLens\n",
    "        \n",
    "        Args:\n",
    "            query: Natural language query\n",
    "            sql: Generated SQL\n",
    "            schema: Database schema as string\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing evaluation results\n",
    "        \"\"\"\n",
    "        # Clean SQL before evaluation\n",
    "        sql = self.clean_sql(sql)\n",
    "        \n",
    "        # Nếu TruLens không khả dụng, trả về cấu trúc giả lập với thông báo\n",
    "        if not self.trulens_available:\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"message\": \"TruLens evaluation skipped (not available)\",\n",
    "                \"all_passed\": True,\n",
    "                \"hallucination_score\": 0.0,\n",
    "                \"relevancy_score\": 0.0,\n",
    "                \"contextual_relevancy_score\": 0.0,\n",
    "                \"correctness_score\": 0.0,\n",
    "                \"syntax_score\": 0.0,\n",
    "                \"metrics_available\": False,\n",
    "                \"reason\": \"OpenAI API key not set or TruLens not installed\",\n",
    "                \"evaluation_framework\": \"TruLens (simulated)\"\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            # Khởi tạo recorder để ghi lại hoạt động\n",
    "            with self.tru.recorder() as recorder:\n",
    "                # Đánh giá relevance\n",
    "                relevance_score = self.relevance.score(\n",
    "                    query=query, response=sql\n",
    "                )\n",
    "                \n",
    "                # Đánh giá groundedness (hallucination)\n",
    "                groundedness_score = self.groundedness.score(\n",
    "                    response=sql, context=[schema]\n",
    "                )\n",
    "                \n",
    "                # Đánh giá context relevance\n",
    "                context_relevance_score = self.context_relevance.score(\n",
    "                    query=query, response=sql, context=[schema]\n",
    "                )\n",
    "                \n",
    "                # Đánh giá SQL correctness\n",
    "                correctness_score = self.sql_correctness.score(\n",
    "                    question=query, schema=schema, sql=sql\n",
    "                )\n",
    "                \n",
    "                # Đánh giá SQL syntax\n",
    "                syntax_score = self.sql_syntax.score(\n",
    "                    sql=sql\n",
    "                )\n",
    "                \n",
    "                # Tính toán all_passed dựa trên ngưỡng\n",
    "                all_passed = (\n",
    "                    relevance_score >= self.thresholds[\"relevance\"] and\n",
    "                    groundedness_score >= self.thresholds[\"groundedness\"] and\n",
    "                    context_relevance_score >= self.thresholds[\"context_relevance\"] and\n",
    "                    correctness_score >= self.thresholds[\"correctness\"] and\n",
    "                    syntax_score >= self.thresholds[\"syntax\"]\n",
    "                )\n",
    "                \n",
    "                # Trả về kết quả đánh giá\n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"all_passed\": all_passed,\n",
    "                    \"metrics_available\": True,\n",
    "                    \"relevancy_score\": relevance_score,\n",
    "                    \"hallucination_score\": 1.0 - groundedness_score,  # Đảo ngược để phù hợp với DeepEval\n",
    "                    \"contextual_relevancy_score\": context_relevance_score,\n",
    "                    \"correctness_score\": correctness_score,\n",
    "                    \"syntax_score\": syntax_score,\n",
    "                    \"evaluation_framework\": \"TruLens\"\n",
    "                }\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"TruLens error: {error_msg}\")\n",
    "            \n",
    "            # Kiểm tra lỗi quyền truy cập mô hình\n",
    "            if \"does not have access to model\" in error_msg:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"error\": \"OpenAI API key valid but no access to required model\",\n",
    "                    \"all_passed\": False,\n",
    "                    \"metrics_available\": False,\n",
    "                    \"reason\": f\"Your OpenAI account does not have access to the required model. Try using a different model.\",\n",
    "                    \"evaluation_framework\": \"TruLens (error)\"\n",
    "                }\n",
    "            \n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"Error evaluating SQL: {error_msg}\",\n",
    "                \"all_passed\": False,\n",
    "                \"metrics_available\": False,\n",
    "                \"reason\": error_msg,\n",
    "                \"evaluation_framework\": \"TruLens (error)\"\n",
    "            }\n",
    "    \n",
    "    def process_query(self, query: str, schema: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Process a natural language query to SQL\n",
    "        \n",
    "        Args:\n",
    "            query: Natural language query\n",
    "            schema: Database schema as string\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing processing results\n",
    "        \"\"\"\n",
    "        result = {\n",
    "            \"original_query\": query,\n",
    "            \"processed_query\": query,\n",
    "            \"source\": \"generated\",\n",
    "            \"is_ambiguous\": False,\n",
    "            \"clarification_needed\": False,\n",
    "            \"sql\": \"\",\n",
    "            \"explanation\": \"\",\n",
    "            \"evaluation\": None\n",
    "        }\n",
    "        \n",
    "        # Step 1: Check FAQ\n",
    "        faq_match = self.match_faq(query)\n",
    "        if faq_match:\n",
    "            result[\"source\"] = \"faq\"\n",
    "            result[\"sql\"] = self.clean_sql(faq_match.get(\"sql\", \"\"))\n",
    "            result[\"explanation\"] = faq_match.get(\"explanation\", \"\")\n",
    "            # Skip ambiguity detection and evaluation for FAQ matches\n",
    "            return result\n",
    "        \n",
    "        # Step 2: Detect ambiguity\n",
    "        is_ambiguous, ambiguity_analysis = self.detect_ambiguity(query, schema)\n",
    "        if is_ambiguous:\n",
    "            result[\"is_ambiguous\"] = True\n",
    "            result[\"ambiguity_analysis\"] = ambiguity_analysis\n",
    "            result[\"clarification_questions\"] = self.generate_clarification_questions(query, ambiguity_analysis, schema)\n",
    "            result[\"clarification_needed\"] = True\n",
    "            return result\n",
    "        \n",
    "        # Step 3: Generate SQL\n",
    "        sql_result = self.generate_sql(query, schema)\n",
    "        if not sql_result.get(\"success\", False):\n",
    "            result[\"error\"] = sql_result.get(\"error\", \"Unknown error generating SQL\")\n",
    "            return result\n",
    "        \n",
    "        result[\"sql\"] = sql_result.get(\"sql\", \"\")\n",
    "        result[\"explanation\"] = sql_result.get(\"explanation\", \"\")\n",
    "        \n",
    "        # Step 4: Evaluate SQL\n",
    "        if result[\"sql\"]:\n",
    "            evaluation = self.evaluate_sql(query, result[\"sql\"], schema)\n",
    "            result[\"evaluation\"] = evaluation\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def process_clarified_query(self, original_query: str, clarification_responses: Dict[str, str], schema: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Process a query after clarification\n",
    "        \n",
    "        Args:\n",
    "            original_query: Original natural language query\n",
    "            clarification_responses: Dict of clarification responses\n",
    "            schema: Database schema as string\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing processing results\n",
    "        \"\"\"\n",
    "        # Update the query based on clarification responses\n",
    "        updated_query = self.update_query_with_clarification(original_query, clarification_responses)\n",
    "        \n",
    "        result = {\n",
    "            \"original_query\": original_query,\n",
    "            \"processed_query\": updated_query,\n",
    "            \"source\": \"clarified\",\n",
    "            \"is_ambiguous\": False,\n",
    "            \"clarification_needed\": False,\n",
    "            \"clarification_responses\": clarification_responses,\n",
    "            \"sql\": \"\",\n",
    "            \"explanation\": \"\",\n",
    "            \"evaluation\": None\n",
    "        }\n",
    "        \n",
    "        # Generate SQL for the updated query\n",
    "        sql_result = self.generate_sql(updated_query, schema)\n",
    "        if not sql_result.get(\"success\", False):\n",
    "            result[\"error\"] = sql_result.get(\"error\", \"Unknown error generating SQL\")\n",
    "            return result\n",
    "        \n",
    "        result[\"sql\"] = sql_result.get(\"sql\", \"\")\n",
    "        result[\"explanation\"] = sql_result.get(\"explanation\", \"\")\n",
    "        \n",
    "        # Evaluate SQL\n",
    "        if result[\"sql\"]:\n",
    "            evaluation = self.evaluate_sql(updated_query, result[\"sql\"], schema)\n",
    "            result[\"evaluation\"] = evaluation\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Blueprint, request, jsonify, current_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
